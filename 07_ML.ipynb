{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "I mange år har MNIST-datasettet med håndskrevne tall blitt brukt både som benchmark og enkel illustrasjon av maskinlæring med bilder. Her bruker vi et veldig lignende datasett, med små svart-hvitt bilder av klesplagg. Vi skulle gjerne brukt et mer realistisk datasett, men grunnet begrensninger i prosessorkraft velger vi å holde oss til det enkle.\n",
    "\n",
    "I dette enkle eksempelet begynner vi med to enkle modeller, som tar utgangspunkt i tabulære data. Hver piksel i bildet har en kolonne, nummerert pixel1-pixel784. Vi starter med en multiclass logistisk modell som predikerer type klesplagg, før vi går over til en neural-net modell som bruker det samme tabulære datasettet. Til slutt omformes dataene til bilder, og vi lager en neural-net modell for bildegjenkjenning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "train = pd.read_csv('/data/kurs/python/fashionmnist/fashion-mnist_train.csv')\n",
    "test = pd.read_csv('/data/kurs/python/fashionmnist/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     int64\n",
       "pixel1    int64\n",
       "pixel2    int64\n",
       "pixel3    int64\n",
       "pixel4    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      2       0       0       0       0       0       0       0       0   \n",
       "1      9       0       0       0       0       0       0       0       0   \n",
       "2      6       0       0       0       0       0       0       0       5   \n",
       "3      0       0       0       0       1       2       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0        30        43         0   \n",
       "3       0  ...         3         0         0         0         0         1   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train.iloc[:,1:].values/255, train.iloc[:,0].values\n",
    "X_test, y_test = test.iloc[:,1:].values/255, test.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "fashion_logreg = LogisticRegression(multi_class='multinomial', solver='saga', penalty='none', max_iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='none',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fashion_logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8585"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = fashion_logreg.predict(X_test)\n",
    "\n",
    "sum(y_pred==y_test) / len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "Her kjører vi en enkel tensorflow/Keras modell på de samme dataene. Fordi dette datasettet er så godt tilrettelagt, har vi ikke behov for preprosessering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = pd.get_dummies(train.iloc[:,0].astype('str'))\n",
    "y_test_onehot = pd.get_dummies(test.iloc[:,0].astype('str'))\n",
    "\n",
    "\n",
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape = (X_test.shape[1],)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(y_train_onehot.shape[1]))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 535,818\n",
      "Trainable params: 535,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.5660 - acc: 0.8028 - val_loss: 0.4272 - val_acc: 0.8521\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.3868 - acc: 0.8620 - val_loss: 0.3679 - val_acc: 0.8699\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.3387 - acc: 0.8782 - val_loss: 0.3369 - val_acc: 0.8791\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_onehot,\n",
    "                    validation_data = (X_test, y_test_onehot),\n",
    "                    batch_size=512,\n",
    "                    epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bildegjenkjenning\n",
    "\n",
    "For å behandle dataene som et bilde trenger vi å omforme de til faktiske bilder, eller rettere sagt numpy-representasjonen av bilder.\n",
    "\n",
    "Under gjør vi egentlig to ting samtidig: Vi bruker reshape til å omforme dimensjonen på X_train, slik at dimensjon 0 er bilder, dimensjon 1 er høyde(?) og dimensjon 2 er bredde. Men vi legger også på en ekstra, tom dimensjon, slik at den endelige dimensjonen er `(60000, 28, 28, 1)` heller enn `(60000, 28, 28)`. Dette er egentlig en teknikalitet, som gir seg utslag ikke i hvordan vi ser bildet, men hvordan verdiene er nøstet i matrisen. Grunnen til at vi gjør det er at det er dette nettverket under forventer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3d = X_train.reshape((60000, 28, 28))[:,:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f38800553c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_3d[1,:,:,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn skjuler litt kompleksitet for oss når Y i et klassifikasjonsproblem kan være tall fra 0 til 9. Keras skjuler også veldig mye kompleksitet, men ikke akkurat dette. Nettverket vi lager må ta som input 28x28 bilder, output må være et en dummyenkodet utgave av Y, med andre ord et array av lengde 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = pd.get_dummies(train.iloc[:,0].astype('str'))\n",
    "\n",
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nettverket er et enkelt sekvensielt nettverk, med to konvolusjonelle lag som flates og etterfølges av to Dense lag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape = (28, 28, 1) ))\n",
    "classifier.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (3, 3)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dense(32, activation='relu'))\n",
    "classifier.add(Dense(y_train_onehot.shape[1]))\n",
    "classifier.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = categorical_crossentropy, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                262208    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 283,434\n",
      "Trainable params: 283,434\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhot_test = pd.get_dummies(test.iloc[:,0].astype('str'))\n",
    "yhot_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = test.iloc[:,1:].to_numpy().reshape((10000, 28, 28))[:,:,:,None]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 38s 629us/step - loss: 0.4653 - acc: 0.8329 - val_loss: 2.0038 - val_acc: 0.8718\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 37s 623us/step - loss: 0.2843 - acc: 0.8988 - val_loss: 2.0843 - val_acc: 0.8664\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 39s 646us/step - loss: 0.2388 - acc: 0.9119 - val_loss: 1.7298 - val_acc: 0.8899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3afec41470>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit( X_train_3d, y_train_onehot,\n",
    "                        epochs=3,\n",
    "                         validation_data=(X_test, yhot_test)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En kjapp forklaring av tapsfunksjonen vår, categorical_crossentropy:\n",
    "\n",
    "Output av funksjonen er en vektor med sannsynligheter for hver klasse, altså tall mellom 0 og 1. Y er en vektor med de sanne klassene, altså en vektor med 0 i alle elementer unntatt den sanne klassen, hvor vektoren er 1.\n",
    "\n",
    "Enhver funksjon som blir lavere desto mindre foskjellen mellom disse to vektorene er, vil kunne fungere som tapsfunksjon. categorical_crossentropy (her illustrert med spesialtilfellet binary_crossentropy) er som følger:\n",
    "\n",
    "\n",
    "$$-\\frac{1}{N}\\sum{y_{i}*log(p_{i}) + (1-y_{i})*log(1-p_{i})}$$\n",
    "\n",
    "For å få en litt bedre forståelse er kanskje denne kalkulatoren grei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01005033585350145"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# i eksempelet er y=1, så tapet blir lavere jo nærmere p er 1\n",
    "p=0.99\n",
    "\n",
    "-(1*math.log(p) + 0*(1-math.log(1-p)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prøv selv\n",
    "\n",
    "Et par forslag til ting å gjøre\n",
    "\n",
    "- Bruk koden fra forrige notebook, og legg til støy på bildene før du trener. Hvordan påvirker dette resultatet?\n",
    "- Etter å ha trent en modell på støyete data, kjør validering på både støyete og ustøyete data. Hvilken fungerer best?\n",
    "- Mange modeller har problemer med overfitting, at modellen lærer seg egenskaper som er spesifikke for treningsdatasettet. Konsulter internettet, og undersøk hvordan `Dropout()`-lag og l2-regularisering kan hjelpe med dette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
